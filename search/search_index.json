{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>Some parts of this page were written using ChatGPT.</p>"},{"location":"bachelors/","title":"Bachelors in Computer Science","text":""},{"location":"bachelors/ads/","title":"Algorithms and Datastructures","text":""},{"location":"bachelors/la/","title":"Linear Algebra","text":""},{"location":"bachelors/tcs/","title":"Theoretical Computer Science","text":""},{"location":"bachelors/tcs/computability/","title":"Computability","text":"<p>Computability is one of the first theories described in computer science. It is used to classify problems into  algorithmically solvable and non-solvable problems. </p>"},{"location":"bachelors/tcs/computability/#infinities","title":"Infinities","text":"<p>At first, we must realize that there are different types of infinity. For example, there are more languages than  there are turing-machines: $|KodTM| &lt; |Pot(\\Sigma^*)|. We know however, that an infinite amount of TMs and an infinite amount of words in the power product exist. Let's start with the following definition. We have the sets \\(A\\) and \\(B\\).</p> <ul> <li>\\(|A| \\leq |B|\\), if an injective function \\(f: A \\to B\\) exists.</li> <li>\\(|A| = |B|\\), if \\(|A| \\leq |B|\\) and \\(|B| \\leq |A|\\) (thus a bijective function exists).</li> <li>\\(|A| &lt; |B|\\), if \\(|A| \\leq |B|\\) and no injective function \\(f: B \\to A\\) exists.</li> </ul> <p>We can make a first observation with the two sets \\(|\\mathbb{N}| = |\\mathbb{N}_{even}|\\). One would think that the first set would be bigger, as the second only contains every second number. However, by the above definitions, we find that they are equivalent as the bijective function \\(f(i)=2i\\) exists. </p> <p>A set is defined as countable if \\(|A|\\) is finite, or if \\(|A| = |\\mathbb{N}|\\).</p>"},{"location":"bachelors/tcs/computability/#hilbert-hotel","title":"Hilbert Hotel","text":"<p>The hilbert hotel is an intuition to compare finite sets. We assume an infinite hotel where the rooms are enumerated  as \\(1,2,...\\). Now, a new guest arrives and we want to find a room for this guest. We tell all the current guests that  they should move into the room next to theirs (\\(i \\to i+1\\)). Room 1 gets empty, and the new guest can move into this  room. </p> <p>Now, if a bus with an infinite amount of guests arrives, we can now tell all the guests in to the room with double  the room number of their current room (\\(i \\to 2i\\)). </p>"},{"location":"bachelors/tcs/computability/#examples-of-infinities","title":"Examples of infinities","text":"<p>We have seen  that \\(\\mathbb{N}\\) is countable. We can also show that \\((\\mathbb{N} - \\{0\\}) \\times (\\mathbb{N} - \\{0\\})\\) is countable as well. We do this by setting up a matrix where the rows and the columns contain the two sets that are multiplied. We then define a canonical order according to the red arrows. With this order, we can enumerate all values present in said set. </p> <p></p> <p>The same way we can also show that \\(\\mathbb{Q}\\) is countable. The rows contain the enumerators, and the columns the dividers. </p>"},{"location":"bachelors/tcs/computability/#diagonalisation","title":"Diagonalisation","text":"<p>A set that is not countable is the real numbers in the interval \\([0,1] = \\{x \\in \\mathbb{R} \\mid 0 \\leq x \\leq x\\}\\). To prove this, we first define the following table where we enumerate real numbers in said interval. </p> <p></p> <p>We define a new entry to the set \\(C\\) as \\(C=0.C_1C_2C_3...\\) with \\(C_i \\neq a_{ii}, c_i \\notin \\{0,9\\}\\) for all \\(i\\).  The new entry, however, is not part of the table as it differs on the diagonal \\(a_{ii}\\) from all other entries (as  we defined that \\(C_i \\neq a_{ii}\\)).</p> <p>Similarly, we define a new table that combines \\(w_i \\in \\Sigma^*_{bool}\\) and \\(M \\in KodTM\\). \\(d_ij = 1\\) if \\(M_i\\)  accepts the word \\(w_j\\).   </p> <p></p> <p>We now construct the diagonal language denoted as \\(L_{diag}\\) is not in \\(\\mathcal{L}_{RE}\\) (the set of recursive  languages). The constructed language does not yet exist in the set \\(L(M_i)\\).</p> \\[ L_{diag} = \\{w \\in (\\Sigma_{bool})^* \\mid w = w_i \\; for \\; some \\; i \\in \\mathbb{N} - \\{0\\} \\; and \\; d_{ii} = 0\\} \\]"},{"location":"bachelors/tcs/computability/#reduction","title":"Reduction","text":""},{"location":"bachelors/tcs/computability/#universal-language","title":"Universal language","text":""},{"location":"bachelors/tcs/computability/#the-postsche-korrespondenzproblem-pkp","title":"The \"Post'sche Korrespondenzproblem\" (PKP)","text":""},{"location":"bachelors/tcs/fsm/","title":"Finite-State machines","text":"<p>A finite-state machine (Endlicher Automat EA) is a state-machine capable of identifying words of a specific regular language. Before getting started, we again need some notions.</p> <p>An EA is defined as a five-tuple \\(M=(Q,\\Sigma,q_0,F,\\Delta)\\).</p> <ul> <li>\\(Q\\): finite set of states</li> <li>\\(\\Sigma\\): input alphabet (symbols that are processed by the EA)</li> <li>\\(q_0 \\in Q\\): starting state</li> <li>\\(F \\subseteq Q\\): set of accepting states</li> <li>\\(\\delta: Q \\times \\Sigma \\to Q\\): the set of all possible state-transitions. The set essentially describes what state   the machine will land in for every possible state combined with every possible symbol of the alphabet.   \\(\\delta(q_i,aw)\\) outputs the state after the machine, which is currently in \\(q_i\\), read the symbol \\(a\\). The function   \\(\\hat{\\delta}\\) is quite similar but outputs the state after reading the entire word</li> <li>\\(Kl[q_i]\\): class of the state \\(q_i\\), describing the form of the words landing in said state</li> </ul> <p>The following additional terms are important.</p> <ul> <li>Configuration: everything that is important for the future - \\((q, aw) \\in Q \\times \\Sigma^*\\) denoting the current   state and the remaining input.</li> <li>Start-configuration: \\((q_0, w) \\in \\{q_0\\} \\times \\Sigma^*\\) denoting the starting state and the full input.</li> <li>End-configuration: every configuration $\\in Q \\times {\\lambda}, the configuration after having read the full   input.</li> <li>Step: expresses the relation \\(|_{\\!\\overline{\\;M\\;}}\\) between two configuration such as   \\((q,w) |_{\\!\\overline{\\;M\\;}} (p,x)\\) with \\(w \\in \\Sigma^*\\), \\(w = ax\\), \\(\\delta(q,a)=p\\).</li> <li>Computation C: describes a sequence \\(C = C_0,C_1,...,C_n\\) of configurations such that \\(C_i |_{\\!\\overline{\\;M\\;}}   C_{i+1}\\). If \\(C_0\\) is the start configuration and \\(C_n\\) the end configuration, C is the computation of M on input x.   \\(C\\) can be accepting or rejecting, depending on the final state \\(q_n\\). If \\(q_n \\in F\\), \\(C\\) is accepting.</li> </ul> <p>Now, let's see a couple of above definition using an example state machine. The following EA accepts the language \\(L=\\{w\\in\\Sigma_{bool}^* \\big|\\, |w|_0 \\: mod\\: 3 = 0\\}\\).</p> <p></p> \\[ \\begin{align*} M &amp;= \\big(Q,\\Sigma,q_0,F,\\delta\\big)\\\\ Q &amp;= \\{q_0,q_1,q_2\\}\\\\ \\Sigma &amp;= \\{0,1\\}\\\\ q_0 &amp;= q_0\\\\ F &amp;= \\{q_0\\} \\end{align*} \\] <p>The following table describes the \\(\\delta\\) function.</p> \\(\\delta\\) 0 1 \\(q_0\\) \\(q_1\\) \\(q_0\\) \\(q_1\\) \\(q_2\\) \\(q_1\\) \\(q_2\\) \\(q_0\\) \\(q_2\\) <p>Now, assume we have input \\(x=01101\\), the following computation is made.</p> \\[ (q_0,01101) \\; |_{\\!\\overline{\\;M\\;}} \\; (q_1,1101) \\; |_{\\!\\overline{\\;M\\;}} \\; (q_1,101) \\; |_{\\!\\overline{\\;M\\;}} \\; (q_1,01) \\; |_{\\!\\overline{\\;M\\;}} \\; (q_2,1) \\; |_{\\!\\overline{\\;M\\;}} \\; (q_2,\\lambda) \\] <p>This is equivalent to \\(\\hat{\\delta}(q_0,x)=q_2\\)</p> <p>The classes of above EA can be written as one generalized description with \\(i\\in\\{0,1,2\\}\\):</p> \\[ Kl[q_i] = \\big\\{w \\in \\Sigma^* \\big| \\, |w|_0 \\; mod \\; 3 = i \\big\\} \\]"},{"location":"bachelors/tcs/fsm/#modularity","title":"Modularity","text":"<p>Big state machines for more complicated languages can be expressed as a cartesian product of multiple smaller state-machines. In below picture, we see three machines with the following definitions:</p> \\[ \\begin{align*} L_{top} &amp;= \\{|w|_a \\; mod \\; 3 = 1\\}\\\\ L_{left} &amp;= \\{ubbv \\; \\mid \\; u,v \\in \\{a,b\\}\\}\\\\ L_{comb} &amp;= \\{|w|_a \\; mod \\; 3 = 1\\ \\land ubbv \\; \\mid \\; u,v \\in \\{a,b\\} \\} \\end{align*} \\] <p></p> <p>The formal definition of the above machine:</p> \\[ \\begin{align*} M &amp;= \\{Q, \\Sigma, \\delta, q_0, F\\}\\\\ Q &amp;= Q_{top} \\times Q_{left}\\\\ q_0 &amp;= (p_0, q_0)\\\\ \\delta &amp;= (\\delta_{top}(q,a),\\delta_{left}(p,a))\\\\ F &amp;= L(M_{top}) \\cap L(M_{left}) \\end{align*} \\]"},{"location":"bachelors/tcs/fsm/#proof-of-non-existence-of-a-shorter-machine","title":"Proof of non-existence of a shorter machine","text":"<p>To prove the non-existence of a smaller machine, we first start by constructing a possible suspected shortest machine. For each state, we then provide a word with which the machine will land in that state after having processed the word.</p> <p>Then, all the provided words must be compared pairwise, and for all pairs, a suffix must be found with which one of the words will be part of the langauge while the other won't.</p> <p>Let's look at an example. Below machine is suspected to be the shortest possible machine detecting the language $L = {w \\in {0,1}^ \\; \\mid \\; a11b, \\; with \\; a,b \\in {a,b}^ \\in }</p> <p></p> <p>We start by constructing three words that land in a different state. See the words \\(\\lambda,1,11\\). Now, the following table compares the words pairwise and provides a suffix \\(z\\) that brings only one of the words into the language.</p> \\(z\\) \\(\\lambda\\) 1 11 \\(\\lambda\\) - 1 \\(\\lambda\\) 1 1 - \\(\\lambda\\) 11 \\(\\lambda\\) \\(\\lambda\\) -"},{"location":"bachelors/tcs/fsm/#proof-of-non-regularity","title":"Proof of non-regularity","text":"<p>Some languages are non-regular. This means that no finite-state machine exists that detects said language. We now discuss three approaches to prove a non-regularity.</p>"},{"location":"bachelors/tcs/fsm/#lemma-33","title":"Lemma 3.3","text":"<p>In case that two words \\(x,y\\) with \\(x \\neq y\\) land in the same state when being processed by an FSM \\(A\\) (\\(\\hat{\\delta}(q_o,x) = \\hat{\\delta}(q_o,y))\\), the following statement is true: \\(xz \\in L(A) \\Leftrightarrow yz \\in L (A)\\). We observe that the machine does not store the past of the word - it always only looks at the current and the future state.</p> <p>We then construct an infinite set of words \\(x_1,x_2,...\\) such that \\(x_iz \\in L\\) but \\(x_jz \\notin L\\).</p> <p>We first constructed an infinite set of words. By definition, there cannot exist a machine that can distinguish all of those words, and there must therefore exist two words (here \\(x_i\\) and \\(x_j\\)) which will arrive in the same state. This, however, was disproven by above statement.</p> <p>Let's look for example at the language $L = {0<sup>{n</sup>2} \\; \\mid \\; n \\in \\mathbb{N}}. We construct the following infinite set of words:</p> <ul> <li>\\(0\\)</li> <li>\\(0^4\\)</li> <li>$...</li> <li>\\(0^{n_i^2} \\cdot 0^{2i+1} = 0^{(n_i+1)^2} \\in L\\)</li> <li>\\(0^{n_j^2} \\cdot 0^{2i+1} = 0^{(n_i+1)^2} \\notin L\\)</li> </ul>"},{"location":"bachelors/tcs/fsm/#pumping-lemma","title":"Pumping Lemma","text":"<p>To prove the non-regularity through the pumping lemma, we search for one specific split of a \\(w \\in L\\) with $|w| \\geq n_0: \\(w =yxz\\). We are free to choose any arbitrary word \\(w\\), but the split must be generalised through below properties.</p> <p>For a regular language, this split must have the following properties:</p> <ul> <li>\\(|yx| \\leq n_0\\)</li> <li>\\(|x| \\geq 1\\)</li> <li>Either all \\(yx^kz (k\\in \\mathbb{N})\\) are in \\(L\\) or none.</li> </ul> <p>Let's look for example at the language \\(L = \\{w \\in \\{a,b,c\\}^* \\; \\mid \\; |w|_{ab} = |w|_{ba}\\}\\). We define our word \\(w = (abc)^{n_0}(bac)^{n_0} \\in L\\). Due to above properties, we know that \\(x\\) must be in the first part of the word, and we can prove the non-regularity by looking at the following two cases:</p> <ul> <li>Case \\(x=c\\): \\(yx^0z = ...abcababc...\\), we now have an additional \\(ba\\) in the word, and thus \\(yx^0z \\notin L\\).</li> <li>Case \\(x\\neq c\\): \\(x\\) contains at least one \\(a\\) or \\(b\\). \\(yx^0z\\) is thus missing at least one \\(ab\\).</li> </ul> <p>Now, because the original word \\(w\\) is in the language \\(L\\) and for any possible split the word will not be in \\(L\\), we have contradiction to the three properties defined above.</p>"},{"location":"bachelors/tcs/fsm/#kolmogorov-complexity","title":"Kolmogorov-Complexity","text":"<p>The non-regularity proof through Kolmogorov-Complexity uses a contradiction. With the information received from the course, we only consider languages over the binary alphabet. It would be possible to extend the proof on other alphabets, but we have not proven this possibility in the course.</p> <p>We therefore first assume that a given language \\(L\\) is regular. We construct a second language \\(L' = \\{y \\in \\{0,1\\} ^* \\mid xy \\in L\\}\\). Then, we look at a word \\(z\\) (not necessarily in \\(L\\)), which is defined with some \\(n \\in \\mathbb {N}\\). \\(z\\) is considered to be the \\(m\\)-th word in \\(L'\\) (is constant for one proof, but can change for different proofs).</p> <p>If we can show that \\(z\\) is always the \\(m\\)-th word in \\(L'\\), we can conclude that \\(z\\) is not depending on \\(n\\) but on the language \\(L\\). The Kolmogorov-Complexity can, in this case, be written as a constant: \\(K(z) \\leq \\lceil log_2(m+1)+c=c'\\).</p> <p>As there are only infinitely many programs of said constant length, but there are infinitely many words in \\(L\\), we have a contradiction and thus \\(L\\) cannot be regular.</p> <p>Let's have an example. We want to prove that the language \\(L_1 = \\{0^{F_n} \\mid n \\in \\mathbb{N}\\}\\) where \\(F_n\\) is the \\(n\\)-th fibonacci number is not regular. \\(F_0 = 0, F_1 = 1, F_{n+1} = F_n + F_{n-1}\\) for \\(n\\geq 1\\).</p> <p>We first assume that \\(L_1\\) is regular (as we will prove the non-regularity by contradiction). We consider the word \\(z = 0^{F_{n+1}-F_n-1}\\), which can be simplified to \\(z = 0^{F_{n-1}-1}, n \\geq 2, n \\in \\mathbb{N}\\). Now, we claim that z is the first word in canonical order in the following language:</p> \\[ L' = \\{y \\in \\{0,1\\}^* \\mid 0^{F_n+1}y \\in L_1\\} \\]"},{"location":"bachelors/tcs/fsm/#non-deterministic-finite-state-machine","title":"Non-deterministic finite-state machine","text":"<p>The Non-deterministic finite-state machines (NEA) are quite similar to the EAs, but with the difference that it can be in multiple states at once; a state can have multiple outgoing transitions for the same symbol. Its formal definition is almost identical: \\(M = (Q, \\Sigma, \\delta, q_0, F)\\) where \\(\\delta\\) has the new definition \\(\\delta: Q \\times \\Sigma \\to Pot(Q)\\).</p> <p>If, after having processed the entire word, at least one final state is an accepting state, the word is in the language. Actually, the sole difference between the NEA and the EA is that some languages have simpler representations as NEAs. Everything else is identical as all NEAs can be converted to an EA.</p> <p>Let's convert the following NEA into an EA.</p> <p></p> <p>We first start constructing a transition table. In the first row we see that there are two possible states after having read the symbol 1. We construct thus a new state that consists of the two possible states, and, in a next step, look at the possible transitions from those states.</p> Q 0 1 \\(q_0\\) \\(q_0\\) \\(\\{q_0,q_1\\}\\) \\(\\{q_0,q_1\\}\\) \\(q_0\\) \\(\\{q_0,q_1,q_2\\}\\) \\(\\{q_0,q_1,q_2\\}\\) \\(\\{q_0,q_2\\}\\) \\(\\{q_0,q_1,q_2\\}\\) \\(\\{q_0,q_2\\}\\) \\(\\{q_0,q_2\\}\\) \\(\\{q_0,q_1,q_2\\}\\) <p>Once all states in the second and third column have been added to the column Q, we can start constructing the EA, which looks as follows. Notice that all states containing the accepting state \\(q_2\\) are accepting states in the EA.</p> <p></p>"},{"location":"bachelors/tcs/languages/","title":"Languages","text":"<p>Before starting to learn about alphabets, words and languages, we require a couple of definitions.</p> <ul> <li>Alphabet \\(\\Sigma\\): Non-empty set containing symbols, letters, numbers, etc.<ul> <li>\\(| \\Sigma |\\): Cardinality, number of elements in alphabet</li> <li>\\(\\mathcal{P}(\\Sigma)\\): Every possible combination of the elements. Note that the empty set must be counted as well</li> </ul> </li> <li>Word \\(w\\): finite string of symbols from \\(\\Sigma\\)<ul> <li>\\(\\lambda\\): empty word, has length 0</li> <li>\\(|w|\\): length of word \\(w\\)</li> <li>\\(|w|_a\\): number of symbol \\(a\\) in word \\(w\\)</li> <li>\\(\\Sigma^*\\): set of all words that can be created using the alphabet \\(\\Sigma\\)</li> </ul> </li> <li>Subword \\(v\\): part of a word. \\(\\exists x,y \\in \\Sigma^* : w = xvy\\) where \\(x\\) is a prefix and \\(y\\) a suffix to \\(v\\)</li> <li>Language \\(L\\): set of words from \\(\\Sigma^*\\) with \\(L \\subseteq \\Sigma^*\\)<ul> <li>\\(L^c\\): complement, all words but the ones from \\(L\\). \\(L^c=\\Sigma^* - L\\)</li> <li>\\(L_\\emptyset\\): the empty language</li> <li>\\(L_\\lambda\\): the language only containing the empty word</li> <li>\\(L_1 \\cdot L_2\\): multiplication of two languages. Multiplying two words means concatenating them. Here, every word   from \\(L_1\\) is combined with those from \\(L_2\\). Example: \\(\\{a,ab\\}\\cdot\\{\\lambda,bc\\}=\\{a,abc,ab,abbc\\}\\)</li> <li>\\(|L|\\): cardinality, number of words in \\(L\\). Note that \\(|L_1\\cdot L_2| \\leq k \\cdot m\\) with \\(k=|L_1|,m=|L_2|\\)</li> <li>\\(L^n\\): exponentiation, multiplication of \\(L\\) with itself \\(n\\)-times. \\(L^{n+1}=L^n\\cdot L\\). Two additional special   cases: \\(L^* = \\bigcup_{i\\in\\mathbb{N}}L^i\\) and \\(L^+ = \\bigcup_{i\\in\\mathbb{N}-\\{0\\}}L^i\\)</li> </ul> </li> </ul>"},{"location":"bachelors/tcs/languages/#language-properties","title":"Language properties","text":"<p>The following chapters discuss some properties and operations for languages and words.</p>"},{"location":"bachelors/tcs/languages/#concatenation","title":"Concatenation","text":"<p>Words can be concatenated. \\(\\Sigma^* \\cdot \\Sigma^* = \\Sigma^*\\). A concrete example is \\(abb\\cdot bbc = abbbbc\\). This operation is associative (\\((a\\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\) )  but not commutative (\\(a \\cdot b \\neq b \\cdot a\\)).</p> <p>\\(a^R\\) is the word \\(a\\) but its letters are in reverse order; \\(a=a_1a_2a_3, a^R=a_3a_2a_1\\).</p>"},{"location":"bachelors/tcs/languages/#cardinal-order","title":"Cardinal order","text":"<p>To get the cardinal order of a language, its words are first sorted by length. Words of the same length are then sorted lexicographically. In mathematical terms, the word \\(u\\) is in front of \\(v\\) if:</p> \\[ |u| &lt; |v| \\lor (|u|=|v| \\land u=xs_iu'\\land v=xs_jv'\\land s_i&lt;s_j) \\] <p>The last two equalities and the last inequality look for the first letter that is different.</p>"},{"location":"bachelors/tcs/languages/#arithmetics-with-languages","title":"Arithmetics with languages","text":"<p>Set operations can be applied on languages such as the subtraction, union, intersection, etc. The same rules apply as to sets, eg.</p> \\[ L_1L_2\\cup L_1L_3 = L_1(L_2\\cdot L_3) \\] <p>An additional definition defines the homomorphism between two languages \\(\\Sigma_1^*,\\Sigma_2^*\\) as every function \\(h: \\Sigma_1^* \\to \\Sigma_2^*\\) with the following properties:</p> <ol> <li>\\(h(\\lambda)=\\lambda\\)</li> <li>\\(h(uv) = h(u) \\cdot h(v)\\) for all \\(u,v\\in \\Sigma_2^*\\).</li> </ol>"},{"location":"bachelors/tcs/languages/#kolmogorov-complexity","title":"Kolmogorov-Complexity","text":"<p>The Kolmogorov-Complexity \\(K\\) is defined as the length of the shortest (Pascal)-program which is able to generate a binary word \\(x\\).</p> <p>The \\(K\\) of the language \\(L_1 = \\{0^n\\}^\\infty_{n=1}\\) for example is defined as \\(K(0^n) \\leq \\lceil log_2(n+1) \\rceil + 1\\) where \\(c\\) is the constant part of the program that is not linked to \\(n\\), and \\(\\lceil log_2(n+1) \\rceil\\) the size of the binary representation of \\(n\\).</p> <p>Let's compare two possible programs for the language \\(L_2 = \\{0^{n^3}\\}^\\infty_{n=1}\\) and their resulting complexity.</p> AB <p>First we define the program A in pseudocode. </p> <pre><code>begin\n  for I = 1 to n^3: write(0)\n end\n</code></pre> <p>In above code, the value of \\(n^3\\) is directly used as an input to the program. Consequently, the Kolmogorov-Complexity is computed as \\(K(A) = \\lceil log(n^3+1) \\rceil + c\\). </p> <p>Our second program B takes advantage of the fact that we do not care about the memory usage of our program. </p> <pre><code>begin\n  J := n\n  J := J * J * J\n  for I = 1 to J: write(0)\nend\n</code></pre> <p>The program above has as an input the value \\(n\\). This is better than seen in program A. This fact is also being reflected in the Kolmogorov-Complexity \\(K(B) = \\lceil log(n+1) \\rceil + d\\).</p>"},{"location":"bachelors/tcs/languages/#non-compressible-words","title":"Non-compressible words","text":"<p>Not all words can be compressed regarding the Kolmogorov-Complexity - this means that for all \\(n \\in \\mathbb{N}-\\{0\\}\\) there exists a word \\(w_n \\in (\\Sigma_{bool})^n\\) such that \\(K(w_n) \\geq |w_n| = n\\). In other words, a word of length \\(n\\) exists for every \\(n\\) that is non-compressible.</p> <p>The language \\(\\{0,1\\}^n\\) containing only words of length \\(n\\) for example has at least two words that are not compressible. The language contains \\(2^n\\) words, but there exist not enough distinct non-empty programs with length strictly smaller than \\(n\\). To be precise, only \\(2^n-2\\) such programs exist.</p> <p>For the language \\(\\{0,1\\}^{\\leq n}\\) this gets even worse as now \\(n^{2+1}-2\\) distinct words exist but there are still only \\(2^n-2\\) distinct programs. Thus, at least half of the words have \\(K(x) \\geq |x|\\).</p> <p>Such words are called random. The same can be said for numbers. A number \\(n\\) is called random if \\(K(n) = K(Bin(n)) \\geq \\lceil log_2(n) \\rceil -1\\).</p>"},{"location":"bachelors/tcs/turingmachine/","title":"Turing-machine","text":"<p>The turing-machine (TM) is a concept of a machine that can detect (almost) all languages. The  Church-Turing-Thesis states that TMs are suitable formalisations of the term \"Algorithm\".</p> <p>It is based on an infinite work tape (which can be seen as an infinite amount of storage) and a finite control (or unformally a \"program\"). The following diagram describes a turing-machine with all its basic components.</p> <p></p>"},{"location":"bachelors/tcs/turingmachine/#definitions","title":"Definitions","text":"<p>The formal definition of a TM is a seven-tuple \\(M=(Q, \\Sigma, \\Gamma, \\delta, q_0, q_{accept}, q_{reject})\\) with the following symbol definitions.</p> <ul> <li>\\(Q\\): finite state set</li> <li>\\(\\Sigma\\): input alphabet</li> <li>\\(\\Gamma\\): working alphabet, \\(\\Sigma \\in \\Gamma, (\u00a2, \\_\\_) \\in \\Gamma - \\Sigma, \\Gamma \\cap Q = \\emptyset\\)</li> <li>\\(\\delta\\): \\(Q- \\{q_{acc}, q_{rej}\\} \\times \\Gamma \\to Q \\times \\Gamma \\times \\{L,R,N\\}\\) where \\(L,R,N\\) are the movements   that the R/W-head is able to do (move left 1, move right 1, neutral)</li> <li>\\(q_0\\): start state</li> <li>\\(q_{accept}\\): accepting state, also \\(q_{acc}\\)</li> <li>\\(q_{reject}\\): rejecting state, also \\(q_{rej}\\)</li> </ul>"},{"location":"bachelors/tcs/turingmachine/#processing-definitions","title":"Processing definitions","text":"<p>A configuration is an element from \\(Konf(M)=\\{\u00a2\\}\\cdot \\Gamma^* \\cdot Q \\cdot \\Gamma^+ \\cup Q\\{\u00a2\\}\\Gamma^+\\). \\(\\Gamma^*\\) describes the tape content to the left of the R/W-head, \\(Q\\) the current head position, and \\(\\Gamma^+\\) everything to the right of the head. The second element of the union describes the special case when the head is on the end mark (where the head should not be able to go further to the left).</p> <p>The start configuration for some input \\(x\\) is \\(q_0\u00a2x = Q_0\\). Further configurations can then be denoted as \\(\u00a2q_1qaw_2\\), where \\(q\\) is the head position, and \\(a\\) is the symbol at said position.</p> <p>Let's consider a step where the head is currently on \\(x_i\\)and will replace said symbol with \\(y\\). There are three possible steps to be taken. Here, we transition from one config \\(Q_j\\) to the next configuration \\(Q_{j+1}\\):</p> <ul> <li>No movement: \\(\u00a2x_1x_2...x_{i-1}qx_ix_{i+1} \\; |_{\\!\\overline{\\;M\\;}} \\; \u00a2x_1x_2...x_{i-1}pyx_{i+1}\\) which is identical   to \\(\\delta(q,x_i)=(p,y,N)\\)</li> <li>Movement left: \\(\u00a2x_1x_2...x_{i-1}qx_ix_{i+1} \\; |_{\\!\\overline{\\;M\\;}} \\; \u00a2x_1x_2...x_{i-2}px_{i-1}yx_{i+1}\\) which is   identical to \\(\\delta(q,x_i)=(p,y,L)\\)</li> <li>Movement right:\\(\u00a2x_1x_2...x_{i-1}qx_ix_{i+1} \\; |_{\\!\\overline{\\;M\\;}} \\; \u00a2x_1x_2...x_{i-1}ypx_{i+1}\\) which is   identical to \\(\\delta(q,x_i)=(p,y,R)\\)</li> </ul> <p>A computation is a sequence of configurations \\(Q_1,Q_2,...\\) with \\(Q_i \\; |_{\\!\\overline{\\;M\\;}} \\; Q_{i+1}\\). A TM can run indefinitely, or stop on \\(w_1qw_2\\) with \\(q \\in \\{q_{acc},q_{rej}\\}\\). The TM can then be accepting if it stops on the accepted state, or rejecting if it stops on a rejecting state or if it is running indefinitely.</p> <p>The accepted language is denoted as \\(L(M) = \\{w \\in \\Sigma^* \\mid q_0 \\notin w\\; |_{\\!\\overline{\\;M\\;}}^* \\; w_1 q_{acc}w_2, \\: w1,w2 \\in \\Gamma^*\\}\\). A language is recursively enumerable (rekursiv aufz\u00e4hlbar) if M is able to tell if a word \\(x\\) is in the language, but might run indefinitely if \\(l \\notin L\\). If M can also always tell if \\(x \\notin L\\), the language is said to be recursively (decidable).</p>"},{"location":"bachelors/tcs/turingmachine/#tm-example","title":"TM example","text":"<p>Let's look at the following example. We have seen previously that the language \\(L = \\{0^{2^n} \\mid n \\geq 1\\}\\) is not regular. With a turing-machine, we are able to detect the language. The following process is a possible solution.</p> <ul> <li>Mark every second \\(0\\)</li> <li>Repeat the above step until only a single \\(0\\) remains.</li> <li>The word is part of \\(L\\) in case above condition applies.</li> </ul>"},{"location":"bachelors/tcs/turingmachine/#multi-tape-turing-machine","title":"Multi-tape turing-machine","text":"<p>The multi-tape turing-machine (MTM) is similar to the TM, but possesses a constant (but arbitrary) number of tapes. One of those tapes is a read-only input tape, while the others are working tapes that are empty at the beginning. See the following visualisation of an MTM.</p> <p></p> <p>The start configuration consists of three parts:</p> <ul> <li>\\(\u00a2w\\$\\) on input tape for an input \\(w\\). The read-head is on \\(\u00a2\\).</li> <li>Empty working tapes, where the R/W-heads are on \\(\u00a2\\)</li> <li>Starting state \\(q_0\\)</li> </ul> <p>The configuration itself is denoted as \\((q,w,i,u_1,i_1,...,u_k,i_k)\\) with the following definitions:</p> <ul> <li>\\(q\\): current state</li> <li>\\(w\\): input</li> <li>\\(i\\): head-position on input</li> <li>\\(u_k\\): content of working band \\(k\\)</li> <li>\\(i_k\\): R/W-head-position on working band \\(k\\)</li> </ul> <p>The transition-function is now defined as:</p> \\[ \\delta: Q \\times (\\Sigma \\cup \\{\u00a2,\\$\\}) \\times \\Gamma^k \\to Q \\times \\{L,R,N\\} \\times (\\Gamma \\times \\{L,R,N\\})^k \\]"},{"location":"bachelors/tcs/turingmachine/#mtm-vs-tm","title":"MTM vs TM","text":"<p>Actually, an MTM is not more powerful than a TM, because we can show that for all MTM, an equivalent TM exists. The TM will be way more complicated, and for one step in an MTM, a series of steps are usually required in a TM. The sole tape of the TM must additionally contain the stored words as well as the position of the heads.</p> <p>A machine \\(A\\) (MTM) is equivalent to a machine \\(B\\) (TM), if for all \\(x \\in \\Sigma^*\\) all the following conditions apply.</p> <ul> <li>\\(A\\) accepts \\(x \\Leftrightarrow B\\) accepts \\(x\\)</li> <li>\\(A\\) rejects \\(x \\Leftrightarrow B\\) rejects \\(x\\)</li> <li>\\(A\\) runs indefinitely on \\(x \\Leftrightarrow B\\) runs indefinitely on \\(x\\)</li> </ul> <p>If above apply, we can say that \\(A\\) and \\(B\\) are equivalent \\(\\Rightarrow L(A) = L(B)\\). However, the other way around \\(A\\) and \\(B\\) are equivalent \\(\\Leftarrow L(A) = L(B)\\) does not apply!</p>"},{"location":"bachelors/tcs/turingmachine/#non-deterministic-turing-machine","title":"Non-deterministic turing-machine","text":"<p>The non-deterministic turing-machine (NTM) is an extension to the TM. It's formally defined as the seven-tuple \\(M = (Q,\\Sigma,\\Gamma,\\delta,q_0,q_{acc},q_{rej})\\) where \\(\\delta\\) has been redefined as  \\(\\delta: (Q-\\{q_acc,q_rej\\}) \\times \\Gamma \\to Pot(Q \\times \\Gamma \\times \\{L,R,N\\})\\). Additionally, delta must make  sure that the tapes are not left. </p> <p>The non-determinism is similar to the one described for the finite-state machines; it is possible for the NTM to  transition into multiple states on the same input, head position and current state. </p> <p>Let M be a NTM, and \\(x \\in \\Sigma^*\\). The NTM can be described as a computational tree \\(T_{M,x}\\) with a root such that: </p> <ul> <li>(i): Nodes are labeled with configurations</li> <li>(ii): root as the input degree 0, labeled with the starting configuration \\(q_0\u00a2x\\)</li> <li>(iii): nodes with label C has children which are all possible following configurations of C.</li> </ul> <p>Analog to the EA, it is also possible to express every NTM as a TM. </p>"},{"location":"bachelors/tcs/turingmachine/#turing-machine-encoding","title":"Turing-machine encoding","text":"<p>All turing-machines can be encoded as binary strings. The following is an arbitrary definition that was seen in the  lectures. Other encodings are possible. </p> \\[ \\begin{align*} M&amp;=(Q,\\Sigma,\\Gamma,\\delta,q_o,q_{acc},q_{rej})\\\\ Q&amp;=\\{q_0,q_1,...,q_{acc},q_{rej}\\} \\;\\; \\Gamma = \\{A_1,...,A_m\\}\\\\ Code(q_1) &amp;= 10^{i+1}1, \\; Code(q_{acc})=10^{m+2}1, \\; Code(q_{rej})=10^{m+3}1\\\\ Code(A_j) &amp;= 110^j11\\\\ Code(N)&amp;=1110111, \\; Code(R)=11100111, \\; Code(L) = 111000111\\\\ Code(\\delta(p,A)=(q,A_n,d))&amp;=\\#Code(p)Code(A)Code(q)Code(A_n)Code(d)\\#\\\\ Code(M) &amp;= \\#0^{m+3}\\#0^\\Gamma\\#\\#Code(Transition1)\\#Code(Transition2)\\#... \\end{align*} \\] <p>We observe that the above encoding is not a binary encoding as the alphabet used is \\(\\{0,1,\\#\\}\\). We therefore  define the function h converting the ternary encoding into a binary encoding and is defined as follows.</p> \\[ \\begin{align*} h&amp;: \\; \\{0,1,\\#\\}^* \\to \\{0,1\\}^*\\\\ Kod(M) &amp;= h(Code(M)) \\end{align*} \\]"},{"location":"data/rtai/","title":"Reliable and Trustworthy Artificial Intelligence","text":""},{"location":"data/rtai/robustness/","title":"Robustness","text":"<p>AIs have typically issues where they make false classifications due modified inputs. Such modifications can happen on data directly, or on the data that is being captured. All in all, it gets more and more important to have AIs that have a certain level of certified robustness giving some securities to the user. This is especially important for security-relevant applications.</p> <p>However, it is generally hard to certify models. And often, certification and improved robustness come with a certain cost regarding the model's performance, especially if those certifications are required to be scalable.</p>"},{"location":"data/rtai/robustness/#attacks","title":"Attacks","text":"<p>AI models have generally weaknesses and can be fooled by attackers. Adding a small amount of noise to an image can for example lead an image recognition model to wrong conclusions, or adding tape at certain spots on a stop sign can lead models to recognise it as a different sign.</p>"},{"location":"data/rtai/robustness/#adversarial-attacks","title":"Adversarial attacks","text":"<p>In adversarial attacks, we generally view two types of attacks:</p> <ul> <li>Targeted Attack where the attacker aims to classify the input as a specific label which differs from the   correct label.</li> <li>Untargeted Attack where the aim is to classify the input as some label that is not the correct one.</li> </ul> <p>The attacks can be further split into white box attacks where the attacker knows the complete model including parameters and the architecture, and black box attacks where the attacker only knows the architectures but has no information about the parameters.</p> <p>The following chapters only discuss white box attacks.</p>"},{"location":"data/rtai/robustness/#fast-gradient-sign-method-fgsm","title":"Fast Gradient Sign Method (FGSM)","text":"<p>This attack uses the gradient descent of the classification model. The loss function of the gradient descent is computed, and the added/removed from the input to form a modified output that should be classified with some other label. The loss function is, however, modified before being added/removed: only the sing of the values is looked at, not the exact value. This lead to better results than taking the gradient itself.</p> <p>First, the perturbation is computed. This is slightly different for the targeted (T) and untargeted (U) mode. In T, the loss function is computed for the target label 3, whereas in U, the original label s is used.</p> \\[ \\eta_{t/s} = \\epsilon \\cdot sign(\\nabla_xloss_{t/s}(x)) \\] <p>where \\(\\epsilon\\) a small value indicating how large the perturbation should be, and \\(sign(g)\\) returning -1 if \\(g &lt; 0\\), 0 if \\(g = 0\\), or 1 if \\(g &gt; 0\\). When applied to an image, \\(x_i\\) is a pixel of the image.</p> <p>As the output of the loss function is passed through the sign function, and then multiplied by \\(\\epsilon\\), the output is guaranteed to stay in the range \\(x \\pm \\epsilon\\). The original paper had a single iteration of this algorithm. If the perturbation did not change the output, the next input was looked at. The algorithm is designed to be fast and simple.</p> <ul> <li>Targeted: \\(x' = x-\\eta_t\\) modifies the input by minimising the loss for the label \\(t\\).</li> <li>Untargeted: \\(x' = x+\\eta_s\\) modifies the input by maximising the loss for the original label \\(s\\).</li> </ul>"},{"location":"data/rtai/robustness/#pgd","title":"PGD","text":""},{"location":"data/rtai/robustness/#adversarial-defenses","title":"Adversarial defenses","text":""},{"location":"data/rtai/robustness/#certification","title":"Certification","text":""},{"location":"data/rtai/robustness/#certified-defense","title":"Certified defense","text":""},{"location":"data/rtai/robustness/#relaxation","title":"Relaxation","text":""},{"location":"data/rtai/robustness/#box","title":"Box","text":""},{"location":"data/rtai/robustness/#milp","title":"MILP","text":""},{"location":"data/rtai/robustness/#deeppoly","title":"DeepPoly","text":""},{"location":"data/rtai/robustness/#diffpoly","title":"DiffPoly","text":""},{"location":"general/placeholder/","title":"Placeholder","text":""},{"location":"security/","title":"Secure and reliable systems","text":"<p>The content of this section is treating courses that are attributed to the major secure and reliable systems.</p>"},{"location":"security/zkp/","title":"Zero-Knowledge Proof","text":"<p>Zero-knowledge proofs are a cryptographic concept and a set of protocols that allow one party, called the prover, to demonstrate to another party, called the verifier, that they possess certain knowledge or information without revealing what that knowledge or information is. In other words, zero-knowledge proofs enable a party to prove that they know a secret or have access to specific data without disclosing the actual content of that secret.</p> <p>The primary goals of zero-knowledge proofs are privacy, security, and trust. They are particularly valuable in situations where one party needs to authenticate themselves or prove their knowledge of something to another party without revealing any sensitive details. Zero-knowledge proofs are used in various applications, including cryptography, cyber-security, and privacy-preserving technologies.</p>"},{"location":"security/zkp/intro-definitions/","title":"Introduction and definitions","text":""},{"location":"security/zkp/intro-definitions/#proofs","title":"Proofs","text":"<p>In general, a proof is used to establish trust between two parties, where a statement is established to be valid. Such a statement is established between two parties:</p> <ul> <li>Prover P (proof writer): Wants to prove a statement</li> <li>Verifier V (proof checker): verifies that the proof is valid</li> </ul> <p>Those parties are interacting with each other in different ways. See the following proof types:</p> <ul> <li>Mathematical proof: One-directional proof, where the prover makes a sequence of logical assertions. All errors   made are detectable by the verifier.</li> <li>Interactive proof (IP): Bi-directional proof, where the messages are randomised and adaptive. Most errors are   detectable by V. Increase proving power (more complex problems can be proven), and increased efficiency.</li> </ul> <p>Now, a general (dirty) description of Zero-Knowledge Proofs could be: Mathematical proofs that were extended with interactions, randomness, and cryptographic assumptions.</p>"},{"location":"security/zkp/intro-definitions/#complexity-classes","title":"Complexity classes","text":"<p>Problems can be separated in different complexity classes. Each class determines how problems can be solved. This mostly concerns the runtime of solving the problem. A Complexity class is a set of languages (problem types with similar properties, e.g. time complexity). A language \\(L\\) is a collection of problems \\(L \\subseteq \\{0,1\\}^*\\), where each problem statement \\(x\\) is a binary encoding of YES/NO \\(x \\in \\{0,1\\}^*\\).</p> <ul> <li>P: Problems are solvable in polynomial time (efficient) and are thus easy to solve. There exists an algorithm \\(M\\)   that is able to say if \\(x \\in L\\) or \\(x \\notin L\\). This is obvious and needs no proof.</li> <li>NP: Problems where it is easy to say that \\(x \\in L\\) using a witness w: \\((x,w) \\in R_L\\) However, there are no   checks in polynomial time that can prove that \\(x \\notin L\\). \\(x \\in L\\) has proof which is easy to check. However, the   witness might be hard to find.</li> <li>IP (interactive proof): Problems are proven using a prover and a verifier. They exchange a certain amount of   messages and the verifier concludes if the proof is valid or not. The verifier must be running polynomial time.   The algorithms are able to verify the two following properties:<ul> <li>Completeness (usually accepts true statements): \\(\\forall x \\in L, Pr_{r,s}[\\langle P(r),V(s)\\rangle (x) = 1]   \\geq \\frac{3}{4}\\)</li> <li>Soundness (usually rejects false statements): \\(\\forall x \\notin L, \\forall P^*, Pr_   {r,s}[\\langle P^*(r),V(s)\\rangle (x) = 1] \\leq \\frac{1}{2}\\)</li> </ul> </li> </ul> <p>In above formulas, \\(Pr_{r,s}\\) denotes a proof system that uses the random values \\(r\\) and \\(s\\), and \\(P^*\\) is any malicious prover. The probability to accept/reject the statements can be changed, as long as acceptance is greater than rejection probability.</p> <p>Increased proving power (seen in the previous chapter) allows for proofs of larger complexity classes.</p>"},{"location":"security/zkp/intro-definitions/#interactive-algorithms-and-interactive-proof","title":"Interactive Algorithms and Interactive Proof","text":"<p>Interactive algorithms are used to perform interactive proofs. A series of \\(k\\) messages are exchanged between the prover and verifiers. Each message \\(a_i\\) contains all \\(i-1\\) previous messages, as well as the input \\(x \\in \\{0,1\\}^*\\) and a random value \\(r\\) or \\(s\\). The complete interaction is denoted as \\(\\langle P^*(r),V(s)\\rangle (x) = a_{k(x)}\\). All messages are stored in the transcript \\((x,a_1,...,a_{k(x)})\\) with \\(a_{k(x)}\\) being the verifiers conclusion if he accepts or rejects the proof.</p> \\[ \\begin{align} a_1 &amp;= P(x,r) \\\\ a_2 &amp;= V(x,a_1,r) \\\\ a_3 &amp;= P(x,a_1,a_2,r) \\\\ \\vdots \\\\ a_{k(x)} &amp;= V(x,a_1,...,a_{k(x)-1},s) \\end{align} \\]"},{"location":"security/zkp/intro-definitions/#graph-isomorphism","title":"Graph Isomorphism","text":"<p>A problem that was seen in the course as an example is the graph-isomorphism problem. To our knowledge, no algorithm exists that can proof if a graph is isomorphic. Isomorphic graphs are graphs that be mapped between each other using some permutation \\(\\pi\\).</p>"},{"location":"security/zkp/intro-definitions/#zero-knowledge-zk","title":"Zero-knowledge (ZK)","text":"<p>The following chapters can be summarised with the following points:</p> <ul> <li>Perfect soundness and perfect zero knowledge cannot be achieved at the same time.</li> <li>Knowledge-soundness also good for \"trivial\" languages</li> <li>Special soundness and SHVZK are those that are used the most.</li> </ul> Soundness Zero Knowledge Proof size Proofs   (short term secrecy) Perfect/statistical Computational Not compressing Arguments (short window for cheating) Computational (short window for cheating) Perfect/statistical &lt;&lt;|x|, |w|"},{"location":"security/zkp/intro-definitions/#perfect-zero-knowledge","title":"Perfect Zero-knowledge","text":"<p>Perfect zero-knowledge is a special case of zero-knowledge proofs, and only a few of those exist. The proof must make sure that the verifier gained no new knowledge - essentially, the verifier should be able to reproduce the proof by themselves. \\((P,V)\\) is a perfect zero-knowledge proof, in case an efficient simulator \\(S\\) exists that is able to reproduce the verifier's view \\((x,s,a_1,...,a_{k(x)})\\).</p> <p>The following example describes a perfect ZKP for graph isomorphism. We have two graphs \\(G_0, G_1\\) that are isomorphic ( there exists a permutation mapping \\(G_0\\) to \\(G_1\\)).</p> <ol> <li>Prover takes a random permutation \\(\\sigma\\), and permutes \\(G_0\\) using this permutation which produces the graph    \\(H\\). \\(H\\) is sent to the verifier.</li> <li>Verifier sends randomly either 0 or 1 to the prover.</li> <li>Prover: if a 0 was received, prover sends \\(\\sigma\\) from step 1 to the verifier. If 1 was received,    \\(\\sigma \\circ \\pi\\) is sent to the verifier (with \\(G_0 = \\pi(G_1)\\)).</li> <li> <p>Verifier now applies the permutation sent by the prover either to \\(G_0\\) or \\(G_1\\) (depending on the random value    sent in step 2.). If the graphs are isomorphic, both ways should result in the random permutation \\(H\\).</p> </li> <li> <p>Completeness is good, as V will always accept if the graphs are isomorphic and the prover knows the permutation.</p> </li> <li>Soundness: say there graphs are not isomorphic, there exists no \\(H\\) that is both isomorphic to \\(G_0\\) and \\(G_1\\) at   the same time. This means that the generated \\(H\\) is only isomorphic to one of the graphs, and in the conversation, V   will only accept the provers response with a probability of 50%.</li> </ol> <p>This perfect zero-knowledge can now be proven using a simulator. Such a simulator is capable of reproducing the verifier's view, which in this case is \\(View_{V^*}^P = (G_0,G_1,\\tau,H,b)\\), with \\(V^*\\) a malicious verifier. The simulator \\(S(V^*,G_0,G_1)\\) then goes through the following steps:</p> <ol> <li>\\(\\tau\\) a random distribution</li> <li>\\(B\\) a random sample of 0 or 1</li> <li>\\(H = \\tau(G_B)\\)</li> <li>Sample \\(b\\) (just as malicious verifier would)</li> <li>If \\(b \\neq B\\), start from begining</li> <li>Output the expected verifiers view.</li> </ol>"},{"location":"security/zkp/intro-definitions/#variations-of-zero-knowledge","title":"Variations of Zero-knowledge","text":"<p>As mentioned before, perfect zero-knowledge is only possible for some languages. Thus, some relaxations are designed.</p> <ul> <li>Black-box zero-knowledge: the simulator only gets black-box access to the malicious verifier, which means that   he has no in-depth access to V, and is only capable of requesting a next symbol. It is noted as \\(S^{V^*}\\).</li> <li>Semi-honest verifier (SHVZK): uses an honest verifier that follows the protocol, but the secret is chosen very   carefully. For this reason, the simulator requires access to said secret.</li> <li>Honest verifier: the simulator has access to an honest verifier, which makes the simulator's job easier. Verifier   is following the protocol.</li> <li>Statistical ZK: the simulator can simulate the malicious verifier if he gets lucky.</li> <li>Computational ZK: the simulator can simulate the malicious verifier if he gets lucky or is hardworking.</li> <li>ZK with auxiliary inputs: additional input is given to prover and verifier (e.g. hint, witness, conversations of   prior protocol). The simulator requires additional input as well.</li> </ul>"},{"location":"security/zkp/intro-definitions/#variations-of-soundness","title":"Variations of Soundness","text":"<ul> <li>Knowledge soundness: \\(\\kappa\\) is the error of how sure one is that the prover is sound. An extractor \\(E\\) extracts   a witness from the prover (somewhat similar to black-box ZK, but the extractor has oracle access to the malicious   prover, producing a witness). The extractor can get both transformations \\(\\tau\\) from the prover (by inputting 0 and   1). The extractor can then extract \\(\\pi = \\tau_1 \\circ \\tau_0^{-1}\\)</li> <li>Interactive arguments: a proof should be convincing. But in this case, arguments can be held between the two   parties, because the protocol only holds against efficient provers. This means that cryptographic assumptions can   prove the soundness.</li> <li>Special soundness: connected to the public coin protocol. A knowledge extractor is used, taking the tree of   transcripts as an input to extract a witness. If such an extractor exists, the protocol with (2k+1) moves is   \\((n_1,...,n_k)-special sound\\). This soundness is typically easier to compute.</li> <li>Perfect, statistical and computational soundness also exist.</li> </ul>"},{"location":"security/zkp/intro-definitions/#public-coin-protocols","title":"Public coin protocols","text":"<p>In general, the verifier publicly sends random \"coin flips\" to the prover. Those are the only messages that are sent by the verifier, and are often also called challenges.</p> <p>The public coin flips can then be arranged in a tree of transcripts. The number of challenges defines the depth of the tree.</p>"},{"location":"security/zkp/non-interactive-zk/","title":"Non-interactive zero-knowledge","text":""},{"location":"security/zkp/sigma/","title":"Sigma protocols","text":"<p>The sigma protocols are the shortest and nicest possible interactive proofs. It is a 3-move public-coin protocol with the following properties:</p> <ul> <li>V always accepts if \\((x,w)\\in R\\)</li> <li>k-special sound, which means that an extractor exists that can extract the witness.</li> <li>SHVZK</li> </ul>"},{"location":"security/zkp/sigma/#commitment-schemes","title":"Commitment schemes","text":"<p>Sigma protocols require commitment schemes. A real world example of a commitment scheme are envelopes: a message is put into a (cryptographic) envelope, that can then no longer be opened by some other entity. The sender committed to a certain message and nobody is able to change this without the others noticing it.</p> <p>The commitment scheme is a collection of three algorithms:</p> <ul> <li>\\(Setup(1^\\delta)\\): public parameters pp are generated. Message space \\(\\mathfrak{M}\\), randomness space   \\(\\mathfrak{R}\\), decommitment space \\(\\mathfrak{D}\\) and commitment space \\(\\mathfrak{C}\\).</li> <li>\\(Commit(pp,m\\in\\mathfrak{M}, r \\gets_\\$ \\mathfrak{R})\\): creates a commitment and a decommitment \\((c,d)\\).</li> <li>\\(Verify(pp,c\\in\\mathfrak{C},d\\in\\mathfrak{D},m\\in\\mathfrak{M})\\): asks question if message was in commitment, and   consequently outputs 0 or 1.</li> </ul> <p>The commitments must be correct and secure. From this, the following points follow.</p> <ul> <li>Correctness: \\(\\forall pp, Verify(pp,Commit(pp,m,r),m)=1\\)</li> <li>Perfect hiding: the commitment should not disclose anything about the message.</li> <li>perfect binding: it must not be possible to change the message.</li> </ul> <p>Hiding and binding also exist as computational hiding and binding. It is not possible to have perfect binding ** and perfect hiding** at the same time.</p>"},{"location":"security/zkp/sigma/#elgamal-encryption","title":"Elgamal encryption","text":"<p>The elgamal is a computationally hiding and perfectly binding example of a commitment scheme. We sample a random prime number h (prime \\(p\\) of order \\(2^\\lambda\\) from \\(\\mathbb{G}\\)) and a secret key \\(s\\), which can then be used to compute \\(g = s \\cdot h\\). \\(\\mathbb{Z}_p\\) is the collection of integers from 0 to p-1, but the operations inside this set are done with mod p.</p> <ul> <li>Setup outputs \\(pp := (\\mathbb{G}, g, h, p)\\)</li> <li>\\(Commit(pp,m\\in\\mathbb{G},r\\gets_\\$\\mathbb{Z}_p)\\): \\(((c_1,c_2),d):=((m+r\\cdot g,r \\cdot h),r)\\)</li> <li>\\(Verify(pp,c,d,m)\\): \\((Commit(pp,m,d)==c)\\)</li> </ul>"},{"location":"security/zkp/sigma/#pedersen-commitment","title":"Pedersen commitment","text":"<p>The pedersen commitment can be plugged into sigma protocols to have a perfect hiding and computationally binding proof, and has the following steps. The protocol is perfectly hiding because \\(r\\cdot h\\) is uniformly random in \\(\\mathbb{G}\\).</p> <ul> <li>Setup generatesagain the same output.</li> <li>\\(Commit(pp,m\\in\\mathbb{Z}_p,r\\gets_\\$\\mathbb{Z}_p)\\): \\(((c_1,c_2),d):=((m\\cdot g,r \\cdot h),r)\\)</li> <li>\\(Verify(pp,c,d,m)\\): \\((Commit(pp,m,d)==c)\\)</li> </ul>"},{"location":"security/zkp/sigma/#example-three-coloring-graph","title":"Example - three-coloring graph","text":"<p>In a three-coloring graph, all adjacent nodes must be of different colours. The IP is as follows (\\(E\\) is the set of edges):</p> <ul> <li>Prover creates a permutation of the graph (the colors change, but the three-coloring stays valid) \\(\\sigma\\).   Additionally, the set of colors \\(col\\) is extracted. A commitment is created: \\((c_u,d_u) \\gets_\\$ Commit(pp,col_u)\\).</li> <li>Prover only sends the commitment \\(c_u\\) to the verifier (\\(u\\) is a vertex).</li> <li>Verifier picks a random edge \\((a,b)\\) from the graph and sends that to the prover</li> <li>Prover sends colors and decommitments of the requested edge to the verifier</li> <li>The verifier checks the commitment and decommitment.</li> </ul> <p>Completeness: the three-coloring stays valid after the permutation. Additionally, the commitment scheme makes sure that the verifier is always capable of correctly verifying the commitments/decommitments.</p> <p>\\(|E|\\)-special soundness: the tree of transcript must contain all edges, and no adjacent nodes must be the same color.</p> <p>There exists a simulator for SHVZK which reproduces the verifier's view that is the following (for a single edge in this example):\\(((c_u)_{u\\in V}(a,b),col_a,d_a,col_b,d_b)\\), and \\(Verify()==1\\) for both nodes a and b, and \\(col_a \\neq col_b\\). The simulator \\(S(pp,G,(a,b))\\) does the following steps (note that the simulator does not know the graph). Note that the simulator is kind of going backwards in the protocol.</p> <ol> <li>Sample two random colors \\(col_a, col_b\\) with \\(col_a \\neq col_b\\)</li> <li>\\((c_a,d_a) \\gets_\\$ Commit(pp,col_a)\\)</li> <li>\\((c_b,d_b) \\gets_\\$ Commit(pp,col_b)\\)</li> <li>All other nodes can be committed to some color (does not matter what, as they stay hidden)</li> <li>Results in the verifier's view</li> </ol>"},{"location":"security/zkp/sigma/#composition","title":"Composition","text":"<p>Often, protocols are repeated to improve the completeness/soundness errors. This might happen in sequence repetition or parallel repetition. The \\(\\Sigma\\)-protocols are often used with parallel repetition. The protocol stays largely unchanged, but instead of sending a single message, multiple messages are sent at once from the prover to the verifier.</p> <p>A 2-sound \\(\\Sigma\\)-protocol for example sends two messages in parallel, which will reduce the soundness error from \\(\\frac{1}{|C|}\\) to \\(\\frac{1}{|C|}\\).</p>"},{"location":"security/zkp/sigma/#and-composition","title":"AND composition","text":"<p>The sigma protocol is capable of combining two relations \\(R\\) and \\(R'\\) into a single relation \\(R_\\land\\). The messages exchanged are similar to the parallel repetition.</p> <p>For this composition to work, both \\(R\\) and \\(R'\\) require that the verifier's challenge space $\\mathfrak{C} is identical.</p> <ol> <li>P then sends first messages using both sigma protocols for \\(R\\) and \\(R'\\)</li> <li>V sends a single challenge \\(c\\). This is valid because the relations must share the challenge space</li> <li>P sends back a message according to the challenge</li> <li>V must then accept both individual verifiers</li> </ol> <p>The AND composition of two sigma protocol is pretty straight forward. The simulator for \\(R_\\land\\) is the combination for the individual relations: \\(S^\\land(x,x',c) := (S(x,c),S'(x',c))\\).</p> <p>The protocol is complete, k-special-sound and SHVZK.</p>"},{"location":"security/zkp/sigma/#or-composition","title":"OR composition","text":"<p>The preconditions for this composition is the same as for the AND composition. There is, however, a difference in how the prover and verifier work together. To start, the prover has only a witness for one of the relations. The prover for the second relation is simulated using a simulator.</p> <ol> <li>P generates message \\(a\\) using prover \\(P_1\\)</li> <li>P generates a random \\(c'\\) from the challenge space</li> <li>P simulates the message \\(a'\\) and \\(z'\\) using \\(c'\\)</li> <li>P sends both \\(a\\) and \\(a'\\) to the verifier</li> <li>V responds with a new random \\(c''\\)</li> <li>P computes \\(c := c''-c'\\), which is then used to compute \\(z\\)</li> <li>P sends \\(z,z'\\) and \\(c,c'\\) to the verifier</li> <li>V checks that both challenges were correctly answered, and \\(c+c' == c''\\)</li> </ol> <p>The protocol is complete, k-special-sound and SHVZK.</p>"},{"location":"security/zkp/sigma/#multi-party-computation","title":"Multi-Party computation","text":"<p>In this protocol \\(\\Pi\\), multiple players \\(P_i\\) want to arrive at a common conclusion (joint function) without leaking any of their private information \\(w_i\\), which also function as the private inputs. The players interact with each other over multiple rounds, and in the end all players should get the output. Additional inputs are a public input \\(x\\), and private randomness \\(r_i\\).</p> <p>The protocol is then specified by a next message function, specifying what message \\(m\\) a player \\(i\\) will send in round \\(j\\) to some other player \\(k, k\\neq i\\). The output is the message that is going to be sent by player \\(P_i\\) in round \\(j+1\\). \\(M\\) are all messages that were received by a certain player. </p> \\[ \\Pi \\big(i,x,w_i,r_i,(M_1,...,M_j)\\big) \\to (m_{j+1,i\\to k}) \\] <p>The protocol is correct, if all players get the correct output. If one input is not in the relation, all players  will get 0. </p> <p>The protocol has also t-privacy. This means that up to \\(t\\) players can be corrupted until the protocol looses its privacy. </p>"},{"location":"security/zkp/sigma/#view","title":"View","text":"<p>A player's view is defined as \\(View_i =(x,w_i,r_i,M_1,...,M_k)\\) where \\(M_j\\) is the set of all messages player \\(P_i\\) received from all other players in a specific round \\(j\\).</p> <p>Two views \\(View_i, View_j\\) are called consistent, if the messages in \\(View_i\\) sent by \\(View_j\\) actually match the  messages that can be computed using \\(View_j\\) and \\(\\Pi\\). This is called local consistency</p> <p>To get global consistency, all combinations of views must be pairwise consistent. If this is the case, there exists a  protocol with input \\(x\\) that matches all views.  </p>"},{"location":"security/zkp/sigma/#sigma-protocol-with-mpc-in-the-head","title":"Sigma-Protocol with MPC in the head","text":"<p>This protocol is called 'in the head' because the prover imagines the graph of players and computes all their witness and their views by themselfs. We now have \\(P(pp,x,w)\\) and \\(V(pp,x\\)) with \\(pp\\) the parameters from the setup function.</p> <ol> <li>P computes all witness for all players \\(w_i\\), with the additional condition that \\(w = w_1 \\oplus ... \\oplus w_n\\)</li> <li>P uses all \\(w_i\\), input \\(x\\) and \\(\\Pi\\) to compute the views</li> <li>P commits to the views, and sends all commitments to the verifier</li> <li>V chooses random players \\(i,j\\)</li> <li>P sends the views and decommitments of \\(i,j\\) to V</li> <li>V runs the verifying function Verfy\\((pp,c_i,d_i,View_i)=1\\), makes sure the views are consistent, and that     \\(P_i = P_j = 1\\)</li> </ol>"},{"location":"security/zkp/sigma/#fiat-shamir-heuristic","title":"Fiat-Shamir Heuristic","text":"<p>The protocol gives full zero knowledge and reduces the number of messages exchanged to 1. Additionally, a hash function \\(H\\) is required, that is modelled as a random oracle. The hash function maps onto the challenge space \\(C\\). Additionally,  \\(H\\) remembers the outputs (uniform random hash) that it has given for previous inputs. Giving the same input again will result in the identical output. </p> <p>Unlike the sigma protocol seen until now, P is generating \\(c = H(x,a)\\). He then sends \\(a\\), \\(c\\), and \\(z\\) to the  verifier. V does the same checks as usual, but also makes sure that \\(c\\) was generated correctly using the common hash function.</p> <p>Some notes: </p> <ul> <li>Hash more than less. If a commitment scheme is involved for example, the public parameters pp should be hashed as well</li> <li>Unsecure for real hash functions (only certain hash-functions can be used for a secure protocol)</li> <li>Signature scheme can be implemented, where the prover signs, and the verifier verifies. This method was used in the   picnic signature scheme that was a candidate as a post-quantum signature scheme. </li> </ul>"},{"location":"security/zkp/sigma/#sigma-protocol-against-malicious-verifiers","title":"Sigma protocol against malicious verifiers","text":"<p>If the verifier generates random challenges, there is no way to gain knowledge. However, non-uniformly-random challenges might leak information.</p> <p>To counter this, a public coin flip protocol is used. The verifier is forced to be honest by generating challenges  together with the prover.</p> <p>The prover generates his challenge \\(c\\) and commits to it. The commitment \\(C\\) is then sent to the verifier. V also  generates a secret \\(c'\\) that he sends back to the prover. P then answers with \\(c\\) and the decommitment \\(d\\), allowing  the verifier to check that \\(c\\) was not changed in the meantime. This procedure can then be interleaved with a message:</p> <ol> <li>P computes \\(a\\), \\(c\\), and \\(C\\), sending \\(a\\) and \\(C\\) to V</li> <li>V generates \\(c'\\) and sends it to P</li> <li>P computes \\(z \\gets_\\$ P_2(x,w,a,c'';p)\\) with \\(c'' := c + c'\\), then sends \\(z,c,d\\) to V</li> <li>V verifies that \\(c\\) has not been changed, and computes the output \\(V(x,a,c'',z)\\)</li> </ol>"},{"location":"security/zkp/zk-arguments-and-proofs/","title":"Zero-knowledge arguments with short proofs","text":""}]}